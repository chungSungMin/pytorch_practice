ResNet의 경우 처음으로 Layer를 깊게 쌓는 모델을 제안하였습니다. ResNet의 가장 큰 제안은 skip-connection을 사용하여 gradient를 최소 1로 보장한다는 것입니다.

이러한 H(x) = F(x) + x 식을 최적화 합니다. 그래서 F(x) 라는 값을 0에 가깝게 학습하도록 유도합니다. F(x)를 0에 가깝도록 유도하는 이유는, ResNet 모델 자체가 깊이가 깊어질 수록 조금씩 학습한다는 철학이 있습니다.

그래서 모델의 깊이가 길어질때, H(x)는 x와 유사하며, 이는 결국 F(x) = 0에 가깝도록 차이만 학습하게 됩니다. 이러한 학습을 "잔차학습" 이라고 합니다.

만일 단순히 H(x) = x 를 학습하려면 H라는 함수가 Identity matrix가 되어야하는데, F(x) + X = x 라면 F(x) 는 0 행렬이면 되기에 보다 유리하다.

즉, skip-connection은 값을 조금씩 줄이도록 유도하는 "잔차학습"을 기반으로 하고 있습니다. 그래서 H'(x) = F'(x) + 1 처럼, 값이 들어왔을떄, 얼마나 변화할럐? 라는 잔차만 학습합니다. **한마디로 가중치를 어떻게 업데이트 해야할지에 대한 가이드를 제공합니다.**


## 정리
Skip-connection은 단순히 기울기를 1보다 크게 유지하는 것 뿐 아니라 ==모델의 Layer기 깊을수록 천천히 학습하고자 하는 철학을 기반==으로 입력에 대해 차이만 학습하도록 유도됩니다. 그래서 깊이가 깊어도 안정적으로 모델을 학습할 수 있어 이전의 모델들에 비해 Layer를 깊게 쌓을 수 있었습니다.